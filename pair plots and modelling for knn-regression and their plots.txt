# Pair plot of a subset of numerical features
numerical_cols_for_pairplot = ['Tuition_USD', 'Living_Cost_Index', 'Duration_Years']
sns.pairplot(intl_edu_df[numerical_cols_for_pairplot])
plt.suptitle('Pair Plot of Selected Numerical Features', y=1.02)
plt.show()

!pip install scikit-learn
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Select features and target variable
features = ['Living_Cost_Index', 'Duration_Years']  # Using relevant numerical features
target = 'Tuition_USD'
X = intl_edu_df[features]
y = intl_edu_df[target]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the KNN Regressor model
knn_model = KNeighborsRegressor(n_neighbors=5) # You can adjust the number of neighbors (k)
knn_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_knn = knn_model.predict(X_test)

# Evaluate the model
mse_knn = mean_squared_error(y_test, y_pred_knn)
r2_knn = r2_score(y_test, y_pred_knn)

print(f"KNN Regressor - Mean Squared Error: {mse_knn}")
print(f"KNN Regressor - R-squared: {r2_knn}")

# Optional: Visualize actual vs. predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_knn, alpha=0.5)
plt.xlabel("Actual Tuition (USD)")
plt.ylabel("Predicted Tuition (USD)")
plt.title("KNN Regressor: Actual vs. Predicted Tuition Costs")
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2) # Plotting a diagonal line
plt.show()

#  k-Means Clustering
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
# Select features for clustering
features_for_clustering = ['Tuition_USD', 'Living_Cost_Index', 'Duration_Years']
X_cluster = intl_edu_df[features_for_clustering]
# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_cluster)
# Determine the optimal number of clusters using the Elbow method
inertia = []
k_range = range(1, 11)  # Trying k from 1 to 10

for k in k_range:
  kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Set n_init to avoid warning
  kmeans.fit(X_scaled)
  inertia.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.figure(figsize=(8, 5))
plt.plot(k_range, inertia, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.xticks(k_range)
plt.grid(True)
plt.show()

# Let's assume we choose k=4 for this example
optimal_k = 4

# Apply K-Means clustering with the chosen number of clusters
kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
intl_edu_df['Cluster'] = kmeans_final.fit_predict(X_scaled)

# Analyze the clusters
print("\nCluster distribution:")
print(intl_edu_df['Cluster'].value_counts())

# View the characteristics of each cluster (e.g., mean of features within each cluster)
print("\nCluster characteristics (mean values):")
print(intl_edu_df.groupby('Cluster')[features_for_clustering].mean())

# Visualize the clusters (example using a scatter plot with two features and color-coding by cluster)
# You might need to choose the two most relevant features for visualization
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Tuition_USD', y='Living_Cost_Index', hue='Cluster', data=intl_edu_df, palette='viridis', legend='full')
plt.title(f'K-Means Clustering (k={optimal_k})')
plt.xlabel('Tuition (USD)')
plt.ylabel('Living Cost Index')
plt.show()

# Select categorical columns
categorical_cols = intl_edu_df.select_dtypes(include='object').columns

# Analyze the distribution of categorical features within each cluster
for col in categorical_cols:
    print(f"\nDistribution of {col} within Clusters:")
    cluster_distribution = intl_edu_df.groupby('Cluster')[col].value_counts().unstack(fill_value=0)
    print(cluster_distribution)

# Optionally, visualize the distributions using bar plots for selected categorical features and clusters

# Visualize the distribution of 'Level' within clusters
plt.figure(figsize=(10, 6))
sns.countplot(x='Cluster', hue='Level', data=intl_edu_df, palette='viridis')
plt.title('Distribution of Level within Clusters')
plt.xlabel('Cluster')
plt.ylabel('Count')
plt.show()

# Visualize the distribution of 'Program' within clusters for the top N programs
top_programs = intl_edu_df['Program'].value_counts().nlargest(5).index
plt.figure(figsize=(14, 7))
sns.countplot(x='Cluster', hue='Program', data=intl_edu_df[intl_edu_df['Program'].isin(top_programs)], palette='viridis')
plt.title('Distribution of Top Programs within Clusters')
plt.xlabel('Cluster')
plt.ylabel('Count')
plt.legend(title='Program', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()

# Visualize the distribution of 'Country' within clusters for the top N countries
top_countries = intl_edu_df['Country'].value_counts().nlargest(5).index
plt.figure(figsize=(14, 7))
sns.countplot(x='Cluster', hue='Country', data=intl_edu_df[intl_edu_df['Country'].isin(top_countries)], palette='viridis')
plt.title('Distribution of Top Countries within Clusters')
plt.xlabel('Cluster')
plt.ylabel('Count')
plt.legend(title='Country', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
